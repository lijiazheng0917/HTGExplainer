{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiazhengli/anaconda3/envs/pt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import dgl\n",
    "import errno\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from dgl.data.utils import download, get_download_dir, _get_dgl_url\n",
    "from pprint import pprint\n",
    "from scipy import sparse\n",
    "from scipy import io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=0):\n",
    "    \"\"\"Set random seed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        Random seed to use\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "def mkdir_p(path, log=True):\n",
    "    \"\"\"Create a directory for the specified path.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path name\n",
    "    log : bool\n",
    "        Whether to print result for directory creation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "        if log:\n",
    "            print('Created directory {}'.format(path))\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path) and log:\n",
    "            print('Directory {} already exists.'.format(path))\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def get_date_postfix():\n",
    "    \"\"\"Get a date based postfix for directory name.\n",
    "    Returns\n",
    "    -------\n",
    "    post_fix : str\n",
    "    \"\"\"\n",
    "    dt = datetime.datetime.now()\n",
    "    post_fix = '{}_{:02d}-{:02d}-{:02d}'.format(\n",
    "        dt.date(), dt.hour, dt.minute, dt.second)\n",
    "\n",
    "    return post_fix\n",
    "\n",
    "def setup_log_dir(args, sampling=False):\n",
    "    \"\"\"Name and create directory for logging.\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : dict\n",
    "        Configuration\n",
    "    Returns\n",
    "    -------\n",
    "    log_dir : str\n",
    "        Path for logging directory\n",
    "    sampling : bool\n",
    "        Whether we are using sampling based training\n",
    "    \"\"\"\n",
    "    date_postfix = get_date_postfix()\n",
    "    log_dir = os.path.join(\n",
    "        args['log_dir'],\n",
    "        '{}_{}'.format(args['dataset'], date_postfix))\n",
    "\n",
    "    if sampling:\n",
    "        log_dir = log_dir + '_sampling'\n",
    "\n",
    "    mkdir_p(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "# The configuration below is from the paper.\n",
    "default_configure = {\n",
    "    'lr': 0.005,             # Learning rate\n",
    "    'num_heads': [8],        # Number of attention heads for node-level attention\n",
    "    'hidden_units': 8,\n",
    "    'dropout': 0.6,\n",
    "    'weight_decay': 0.001,\n",
    "    'num_epochs': 200,\n",
    "    'patience': 100\n",
    "}\n",
    "\n",
    "sampling_configure = {\n",
    "    'batch_size': 20\n",
    "}\n",
    "\n",
    "def setup(args):\n",
    "    args.update(default_configure)\n",
    "    set_random_seed(args['seed'])\n",
    "    args['dataset'] = 'ACMRaw'\n",
    "    args['device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    args['log_dir'] = setup_log_dir(args)\n",
    "    return args\n",
    "\n",
    "def setup_for_sampling(args):\n",
    "    args.update(default_configure)\n",
    "    args.update(sampling_configure)\n",
    "    # set_random_seed()\n",
    "    args['device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    args['log_dir'] = setup_log_dir(args, sampling=True)\n",
    "    return args\n",
    "\n",
    "def get_binary_mask(total_size, indices):\n",
    "    mask = torch.zeros(total_size)\n",
    "    mask[indices] = 1\n",
    "    return mask.byte()\n",
    "\n",
    "def load_acm(remove_self_loop):\n",
    "    url = 'dataset/ACM3025.pkl'\n",
    "    data_path = get_download_dir() + '/ACM3025.pkl'\n",
    "    download(_get_dgl_url(url), path=data_path)\n",
    "\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    labels, features = torch.from_numpy(data['label'].todense()).long(), \\\n",
    "                       torch.from_numpy(data['feature'].todense()).float()\n",
    "    num_classes = labels.shape[1]\n",
    "    labels = labels.nonzero()[:, 1]\n",
    "\n",
    "    if remove_self_loop:\n",
    "        num_nodes = data['label'].shape[0]\n",
    "        data['PAP'] = sparse.csr_matrix(data['PAP'] - np.eye(num_nodes))\n",
    "        data['PLP'] = sparse.csr_matrix(data['PLP'] - np.eye(num_nodes))\n",
    "\n",
    "    # Adjacency matrices for meta path based neighbors\n",
    "    # (Mufei): I verified both of them are binary adjacency matrices with self loops\n",
    "    author_g = dgl.from_scipy(data['PAP'])\n",
    "    subject_g = dgl.from_scipy(data['PLP'])\n",
    "    gs = [author_g, subject_g]\n",
    "\n",
    "    train_idx = torch.from_numpy(data['train_idx']).long().squeeze(0)\n",
    "    val_idx = torch.from_numpy(data['val_idx']).long().squeeze(0)\n",
    "    test_idx = torch.from_numpy(data['test_idx']).long().squeeze(0)\n",
    "\n",
    "    num_nodes = author_g.number_of_nodes()\n",
    "    train_mask = get_binary_mask(num_nodes, train_idx)\n",
    "    val_mask = get_binary_mask(num_nodes, val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes, test_idx)\n",
    "\n",
    "    print('dataset loaded')\n",
    "    pprint({\n",
    "        'dataset': 'ACM',\n",
    "        'train': train_mask.sum().item() / num_nodes,\n",
    "        'val': val_mask.sum().item() / num_nodes,\n",
    "        'test': test_mask.sum().item() / num_nodes\n",
    "    })\n",
    "\n",
    "    return gs, features, labels, num_classes, train_idx, val_idx, test_idx, \\\n",
    "           train_mask, val_mask, test_mask\n",
    "\n",
    "def load_acm_raw(remove_self_loop):\n",
    "    assert not remove_self_loop\n",
    "    # url = 'dataset/ACM.mat'\n",
    "    # data_path = get_download_dir() + '/ACM.mat'\n",
    "    # download(_get_dgl_url(url), path=data_path)\n",
    "    data_path = './ACM.mat'\n",
    "\n",
    "    data = sio.loadmat(data_path)\n",
    "    p_vs_l = data['PvsL']       # paper-field?\n",
    "    p_vs_a = data['PvsA']       # paper-author\n",
    "    p_vs_t = data['PvsT']       # paper-term, bag of words\n",
    "    p_vs_c = data['PvsC']       # paper-conference, labels come from that\n",
    "\n",
    "    # We assign\n",
    "    # (1) KDD papers as class 0 (data mining),\n",
    "    # (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "    # (3) SIGCOMM and MOBICOMM papers as class 2 (communication)\n",
    "    conf_ids = [0, 1, 9, 10, 13]\n",
    "    label_ids = [0, 1, 2, 2, 1]\n",
    "\n",
    "    p_vs_c_filter = p_vs_c[:, conf_ids]\n",
    "    p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "    p_vs_l = p_vs_l[p_selected]\n",
    "    p_vs_a = p_vs_a[p_selected]\n",
    "    p_vs_t = p_vs_t[p_selected]\n",
    "    p_vs_c = p_vs_c[p_selected]\n",
    "\n",
    "    hg = dgl.heterograph({\n",
    "        ('paper', 'pa', 'author'): p_vs_a.nonzero(),\n",
    "        ('author', 'ap', 'paper'): p_vs_a.transpose().nonzero(),\n",
    "        ('paper', 'pf', 'field'): p_vs_l.nonzero(),\n",
    "        ('field', 'fp', 'paper'): p_vs_l.transpose().nonzero()\n",
    "    })\n",
    "\n",
    "    features = torch.FloatTensor(p_vs_t.toarray())\n",
    "\n",
    "    pc_p, pc_c = p_vs_c.nonzero()\n",
    "    labels = np.zeros(len(p_selected), dtype=np.int64)\n",
    "    for conf_id, label_id in zip(conf_ids, label_ids):\n",
    "        labels[pc_p[pc_c == conf_id]] = label_id\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    float_mask = np.zeros(len(pc_p))\n",
    "    for conf_id in conf_ids:\n",
    "        pc_c_mask = (pc_c == conf_id)\n",
    "        float_mask[pc_c_mask] = np.random.permutation(np.linspace(0, 1, pc_c_mask.sum()))\n",
    "    train_idx = np.where(float_mask <= 0.2)[0]\n",
    "    val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "    test_idx = np.where(float_mask > 0.3)[0]\n",
    "\n",
    "    num_nodes = hg.number_of_nodes('paper')\n",
    "    train_mask = get_binary_mask(num_nodes, train_idx)\n",
    "    val_mask = get_binary_mask(num_nodes, val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes, test_idx)\n",
    "\n",
    "    return hg, features, labels, num_classes, train_idx, val_idx, test_idx, \\\n",
    "            train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(dataset, remove_self_loop=False):\n",
    "    if dataset == 'ACM':\n",
    "        return load_acm(remove_self_loop)\n",
    "    elif dataset == 'ACMRaw':\n",
    "        return load_acm_raw(remove_self_loop)\n",
    "    else:\n",
    "        return NotImplementedError('Unsupported dataset {}'.format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/jiazhengli/.dgl/ACM3025.pkl from https://data.dgl.ai/dataset/ACM3025.pkl...\n",
      "dataset loaded\n",
      "{'dataset': 'ACM',\n",
      " 'test': 0.7024793388429752,\n",
      " 'train': 0.19834710743801653,\n",
      " 'val': 0.09917355371900827}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2423185/2341893272.py:111: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, num_classes, train_idx, val_idx, test_idx, train_mask, val_mask, test_mask = load_data('ACM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=3025, num_edges=29281,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
